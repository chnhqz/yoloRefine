### yolo是什么？
>**YOLO**是目标检测模型

目标检测是计算机视觉中比较简单的任务，用来在一张图片中找到**某些特定的物体**，检测
不仅要求我们识别这些物体的**种类**，同时要求我们标出这些物体的**位置**。
![](./img/yolo/截屏2023-10-27 21.50.31.png)
很显然，整体上这三类任务从易到难，我们要讨论的目标检测位于中间。
前面的分类任务是我们做目标检测的基础。

YOLO的全称是you only look once，指只需要浏览一次就可以识别出图中的物体的类别和位置。

因为只需要看一次，YOLO被称为Region-free方法，相比于Region-based方法，YOLO不需要提前找到可能存在目标的Region。

也就是说，一个典型的Region-base方法的流程是这样的：先通过计算机图形学（或者深度学习）的方法，对图片进行分析，找出若干个可能存在物体的区域，将这些区域裁剪下来，放入一个图片分类器中，由分类器分类。

因为YOLO这样的Region-free方法只需要一次扫描，也被称为单阶段（1-stage）模型。Region-based方法方法也被称为两阶段（2-stage）方法。

### YOLO原理

在这之前，我们再重申一下我们的任务。我们的目的是在一张图片中找出物体，并给出它的类别和位置。
目标检测是基于监督学习的，每张图片的监督信息是它所包含的N个物体，每个物体的信息有五个，
分别是物体的中心位置(x,y)和它的高(h)和宽(w)，最后是它的类别。

**YOLO**(You Only Look Once)是继RCNN，
fast-RCNN和faster-RCNN之后，Ross Girshick针对DL目标检测速度问题提出的另一种框架，
其核心思想是生成RoI+目标检测两阶段（two-stage）算法用一套网络的一阶段（one-stage）算法替代，
直接在输出层回归bounding box的位置和所属类别。

之前的物体检测方法首先需要产生大量可能包含待检测物体的先验框, 
然后用分类器判断每个先验框对应的边界框里是否包含待检测物体，以及物体所属类别的概率或者置信度，
同时需要后处理修正边界框，最后基于一些准则过滤掉置信度不高和重叠度较高的边界框，进而得到检测结果。
这种基于先产生候选区再检测的方法虽然有相对较高的检测准确率，但运行速度较慢。

YOLO创造性的将物体检测任务直接当作回归问题（regression problem）来处理，
将候选区和检测两个阶段合二为一。只需一眼就能知道每张图像中有哪些物体以及物体的位置。
下图展示了各物体检测系统的流程图。
![](./img/yolo/截屏2023-10-27 21.55.22.png)
YOLO 的预测是基于整个图片的，并且它会一次性输出所有检测到的目标信息，包括类别和位置。

就好像捕鱼一样，R-CNN是先选好哪里可能出现鱼，而YOLO是直接一个大网下去，把所有的鱼都捞出来。

先假设我们处理的图片是一个正方形。

YOLO的第一步是分割图片，它将图片分割为 $ s^2 $ 个grid，每个grid的大小都是相等的，像这样：
![](./img/yolo/截屏2023-10-27 21.56.32.png)

如果我们让每个框只能识别一个物体，且要求这个物体必须在这个框之内，那么YOLO就变成里很蠢的滑窗法。YOLO的聪明之处在于，它只要求这个物体的中心落在这个框之中。这意味着，我们不用设计非常大的框，因为我们只需要让物体的中心在这个框中就可以了，而不是必须要让整个物体都在这个框中。

具体实现，我们要让这个 $S^2$个框都预测出 $B$个 bounding box，这个bounding box有5个量，分别是物体的中心位置 $(x,y)$和它的高 $(h)$和宽 $(w)$，以及这次预测的置信度。

每个框不仅只预测 $B$个bounding box，它还要负责预测这个框中的物体是什么类别，这里的类别用 one-hot编码表示。

注意⚠️，虽然一个框有多个bounding boxes，但是只能识别一个物体，因此每个框需要预测物体的类别，而bounding box不需要。

也就是说，如果我们有 $S^2$个框，每个框的bounding boxes个数为B，分类器可以识别出C种不同的物体，那么所有整个 ground truth的长度为：
$$
S\times S\times (B\times 5 + C)
$$
先看看这些bounding box显示出来是什么样子：

<img src="/Users/huangqiuzhao/python/yoloRefine/md文件/img/yolo/截屏2023-10-29 19.46.34.png" style="zoom:50%;" />

在上面的例子中，图片被分成了49个框，每个框预测2个bounding box，因此上面的图中有98个bounding box。

可以看到这些BOX中有的边框比较粗，有的比较细，这是**置信度**不同的表现，置信度高的比较粗，置信度低的比较细。

bounding box可以锁定物体的位置，这要求它输出四个关于位置的值，分别是x,y,h和w。我们在处理输入的图片的时候想让图片的大小**任意**，这一点对于卷积神经网络来说不算太难，但是，如果输出的位置坐标是一个**任意的正实数**，模型很可能在**大小不同**的物体上**泛化能力**有很大的差异。

这时候当然有一个常见的套路，就是对数据进行归一化，让连续数据的值位于0和1之间。

对于x和y而言，这相对比较容易，毕竟x和y是物体的中心位置，既然物体的中心位置在这个grid之中，那么只要让真实的**x除以bounding box的宽度**，让真实的**y除以bounding box的高度**就可以了。

但是h和w就不能这么做了，因为一个物体很可能远大于grid的大小，预测物体的高和宽很可能大于bounding box的高和宽，这样w除以bounding box的宽度，h除以bounding box的高度依旧**不在0和1之间**。

解决方法是让**w除以整张图片的宽度**，**h除以整张图片的高度**。

下面的例子是一个448*448的图片，有3*3的grid，展示了计算x,y,w,h的真实值（ground truth）的过程：

![](/Users/huangqiuzhao/python/yoloRefine/md文件/img/yolo/截屏2023-10-29 19.49.51.png)

Confidence 的计算公式：
$$
C=Pr(obj)*IOU^{pred}_{truth}
$$
这个IOU的全称是intersection over union，也就是交并比，它反应了两个框框的相似度。

<img src="/Users/huangqiuzhao/python/yoloRefine/md文件/img/yolo/截屏2023-10-29 19.52.00.png" style="zoom:50%;" />

$IOU^{pred}_{truth}$的意思是预测的bounding box 和真实的物体位置的交并比。

$Pr(obj)$是一个grid有物体的概率，在有物体的时候ground truth 为1，反之为0。

这个$IOU^{pred}_{truth}$非常有意思，因为它的ground truth不是确定的，这导致虽然$Pr(obj)$的ground truth是确定的，但是bounding box的confidence的ground truth是不确定的。

一个不确定的ground truth有什么用呢？

想象这样的问题：老师问小明1+1等于几。小明说等于2，老师又问你有多大的把握你的回答是对的，小明说有80%

这里的80%就是confidence。

confidence主要有两个作用，在后面我会一一介绍。

现在，我们根据上面大雁的图片计算一下样本的groun truth：

<img src="/Users/huangqiuzhao/python/yoloRefine/md文件/img/yolo/截屏2023-10-29 19.49.51.png" style="zoom:50%;" />

首先，这里有9个grid，每个grid有两个bounding box，每个bounding box有5个预测值，假设分类器可以识别出3中物体，那么ground truth的总长度为 S×S×(B×5+C)=3×3×(2×5+3)=117我们假定大雁的类别的one-hot为100，另外两个是火鸡和特朗普，分别是010和001.我们规定每个grid的ground truth的顺序是confidence, x, y, w, h, c1, c2, c3那么第一个（左上角）grid的ground truth应该是：0, ?, ?, ?, ?, ?, ?, ?实际上除了最中间的grid以外，其他的grid的ground truth都是这样的。这里的"?"的意思是，随便是多少都行，我不在乎。在下面我们会看到，我们**不会**对这些值计算损失函数。中间的ground truth应该是：iou, 0.48, 0.28, 0.50, 0.32, 1, 0, 0iou要根据x, y, w, h的预测值现场计算。

这样看似可以让每个grid找到负责的物体，并把它识别出来了。但是还存在一个不得不考虑的问题，如果物体很大，而框框又很小，一个物体被多个框框识别了怎么办？

这里，我们要用到一个叫做**非极大值抑制**Non-maximal suppression(NMS)的技术。

这个NMS还是基于交并比实现的。

<img src="/Users/huangqiuzhao/python/yoloRefine/md文件/img/yolo/截屏2023-10-29 20.04.16.png" style="zoom:50%;" />

例如在上面狗狗的图里，B1,B2,B3,B4这四个框框可能都说狗狗在我的框里，但是最后的输出应该只有一个框，那怎么把其他框删除呢？

这里就用到了我们之前讲的confidence了，confidence预测**有多大的把握这个物体在我的框里**，我们在同样是检测狗狗的框里，也就是B1,B2,B3,B4中，选择confidence最大的，把其余的都删掉。

也就是只保留B1.

但是这里还有一个引人深思的问题，为什么confidence的定义是 $C=Pr(obj)*IOU^{pred}_{truth}$ ，直接用 $Pr(obj)$ 不行吗，直接用 $Pr(obj)$ 的话就可以把ground truth确定下来，训练的时候就方便多了。

这里有一个非常非常鸡贼的技巧！

理论上只用 $Pr(obj)$ 也可以选出应该负责识别物体的grid，但是可能会**不太精确**。这里我们训练的目标是预测 $C=Pr(obj)*IOU^{pred}_{truth}$，我们的想法是让本来不应该预测物体的grid的confidence尽可能的小，既然 $Pr(obj)$ 的效果不太理想，那我就让 $IOU^{truth}_{pred}$ **尽可能小**。

为什么**真正的最中间的grid**的confidence往往会比较大呢？

因为我们的bounding boxes是用**中点坐标+宽高**表示的，每个grid预测的bounding box都要求其中心在这个grid内，那么如果不是最中间的grid，其他的grid的**IOU自然而言就会比较低**了，因此相应的confidence就降下来了。

现在，我们知道了哪个是应该保留的bounding boxes了，但是还有一个问题，我们是怎么判断出这几个bounding boxes**识别的是同一个物体**的呢？

这里用到NMS的技巧，我们首先判断这几个grid的类别是不是相同的，假设上面的B1，B2，B3和B4识别的都是狗狗，那么进入下一步，我们保留B1，然后判断B2，B3和B4要不要删除。

我们把B1成为**极大bounding box**，计算极大bounding box和其他几个bounding box的IOU，如果超过一个阈值，例如0.5，就认为这**两个bounding box实际上预测的是同一个物体**，就把其中**confidence比较小**的删除。

最后，我们结合极大bounding box和grid识别的种类，判断图片中有什么物体，它们分别是什么，它们分别在哪。

<img src="/Users/huangqiuzhao/python/yoloRefine/md文件/img/yolo/截屏2023-10-29 20.08.08.png" style="zoom:50%;" />

我们刚才说confidence有两个功能，一个是用来极大值抑制，另一个就是在最后输出结果的时候，将某个bounding box的confidencd和这个bounding box所属的grid的类别概率相乘，然后输出。

举个例子，比如某个grid中的某个bounding box预测到了一个物体，将这个bounding box送入神经网络（其实是整张图片一起送进去的，我们这样说是为了方便），然后神经网络**对bounding box**说，你这里有一个物体的概率是0.8.然后神经网络又**对grid**说，你这个grid里物体最可能是狗，概率是0.7。

那最后这里是狗的概率就是 0.8×0.7=0.56 。

yolo的损失函数是这样的：

<img src="/Users/huangqiuzhao/python/yoloRefine/md文件/img/yolo/截屏2023-10-29 20.09.20.png" style="zoom:50%;" />

首先，$1^{obj}_{ij}$代表的是这个grid里有没有物体，如果这个grid没有物体，为0，反之为1.

$1^{noobj}_{ij}$相反。

说白了就是，当一个grid有物体的时候，损失函数计算第1，2，3，5项，当grid里没有物体的时候计算第4项。

损失函数一共有5项，我们一项一项的分析。

首先的是中心坐标的损失函数，用了我们最熟悉的均方误差MSE，这个很好理解。

然后是高和宽，没有简单的用MSE，而是用平方根的MSE，这是为什么呢？

第一个原因是更容易优化，但是还有更重要的原因：

看下面的表格：

<img src="/Users/huangqiuzhao/python/yoloRefine/md文件/img/yolo/截屏2023-10-29 20.11.43.png" style="zoom:50%;" />

首先，我们只考虑var1和var2在0和1之间。当var1和var2都很小的时候，也即是w和h都很小，意味着这个物体很小，那么我们应该尽量放大一些损失函数，让模型在识别小物体的时候准确一点。当var1和var2都很大，意味着这个物体也很大，甚至可能已经布满整张图片了，这时我么可以减小一些损失函数，毕竟很大的物体不需要很高的精度。

一句话，使用平方根的MSE而不是MSE其实就是像让模型**对小尺度的物体更敏感**。或者说，对大的和小的物体同样敏感。

接下来的三项都使用了MSE，其实用交叉熵可能会更好，但是这些细节就不追究了。

其中第4项是用来判断一个bounding box中究竟有没有物体的。

接下来的问题是 $λ_{coord}$ 和 $λ_{noobj}$ 都应该取怎样的值，为什么这样设计？论文中给出的答案是  $λ_{coord}=5$和 $λ_{noobj}=0.5$ ，也就是放大第一项和第二项的损失函数，缩小第四项的损失函数。这样做的原因是让梯度更稳定，如果grid中不含有物体，它对1，2，3，5项没有影响，如果调节第四项，会让含有物体的grid的confidence发生改变，这可能使其他项的梯度剧烈变化，从而带来模型上的不稳定。因此，我们放大第一项和第二项，缩小第四项。












